{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9e51a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|████████████████████████████████████████████████████████| 6.80M/6.80M [00:04<00:00, 1.60MB/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [01:51<00:00,  4.27s/it]\n"
     ]
    },
    {
     "ename": "InvalidTextRepresentation",
     "evalue": "invalid input syntax for type integer: \"Decision - Unanimous\"\nLINE 64: ...ight' ,'L' ,0 ,0 ,1 ,1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0.0 ,'Decision ...\n                                                              ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidTextRepresentation\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 400\u001b[0m\n\u001b[0;32m    397\u001b[0m cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# Insert data to the table\u001b[39;00m\n\u001b[1;32m--> 400\u001b[0m \u001b[43minsert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_to_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# Commit the transaction\u001b[39;00m\n\u001b[0;32m    403\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "File \u001b[1;32m~\\UFC_functions.py:379\u001b[0m, in \u001b[0;36minsert_data\u001b[1;34m(cur, data, values)\u001b[0m\n\u001b[0;32m    312\u001b[0m insert_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124mINSERT INTO mma (\u001b[39m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124m    ID,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124mVALUES (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m);\u001b[39m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Execute the SQL statement to insert data into the table\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutemany\u001b[49m\u001b[43m(\u001b[49m\u001b[43minsert_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInvalidTextRepresentation\u001b[0m: invalid input syntax for type integer: \"Decision - Unanimous\"\nLINE 64: ...ight' ,'L' ,0 ,0 ,1 ,1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0.0 ,'Decision ...\n                                                              ^\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import psycopg2 as pg2\n",
    "\n",
    "from UFC_functions import get_fighters_df\n",
    "from UFC_functions import fixing_collumns\n",
    "from UFC_functions import get_stats_ufc\n",
    "from UFC_functions import get_stats_ufcstats\n",
    "from UFC_functions import insert_data\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "'''\n",
    "We want to open the page, deny cookies, click past events and get what events we have,\n",
    "We dont need to interact with the load more button.\n",
    "\n",
    "'''\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install())) # Open chrome\n",
    "\n",
    "driver.get(\"https://www.ufc.com/events\") # Go to the webpage\n",
    "\n",
    "driver.find_element(By.XPATH, '//*[@id=\"onetrust-reject-all-handler\"]').click() # Deny cookies\n",
    "\n",
    "sleep(1 + random.random())\n",
    "\n",
    "driver.find_element(By.XPATH, '//*[@id=\"block-mainpagecontent\"]/div/div/div[5]/div/ul/li[2]/a/strong').click() # Past events\n",
    "\n",
    "# Get the page source and create a BeautifulSoup object\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "# Find all anchor tags with a \"Recap\" button\n",
    "recap_buttons = soup.find_all(\"a\", class_=\"e-button--white\")\n",
    "recap_links = [button.get(\"href\") for button in recap_buttons if \"Recap\" in button.get_text()]\n",
    "recap_links = ['https://www.ufc.com' + link for link in recap_links]\n",
    "\n",
    "driver.close()\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install())) # Open chrome\n",
    "driver.get(recap_links[0]) # Go to the webpage\n",
    "driver.find_element(By.XPATH, '//*[@id=\"onetrust-reject-all-handler\"]').click() # Deny cookies\n",
    "sleep(1 + random.random())\n",
    "\n",
    "# Get the page source and create a BeautifulSoup object\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "\n",
    "# Returns a elements which have '/athlete/' in them and also span elements for given and family name\n",
    "a_elements = soup.select('a[href*=\"/athlete/\"] span.c-listing-fight__corner-given-name, a[href*=\"/athlete/\"] span.c-listing-fight__corner-family-name')\n",
    "\n",
    "# Given and family name span elements\n",
    "temp_list = [elem.text for elem in a_elements]\n",
    "\n",
    "\n",
    "'''\n",
    "The athletes which have more than one family or given name have them in the same span\n",
    "So we can use the line below to get pairs of Given and Family name\n",
    "\n",
    "'''\n",
    "# Use a list comprehension to concatenate each pair of elements into a single string\n",
    "fought_recently = [temp_list[i] + ' ' + temp_list[i+1] for i in range(0, len(temp_list), 2)]\n",
    "    \n",
    "driver.close()\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "with open(\"D:/data_projects/mma/updates/athletes_names_list.pickle\", \"rb\") as file:\n",
    "    names = pickle.load(file)\n",
    "    \n",
    "    \n",
    "# Names that are in the fought recently list but not in the fought past list\n",
    "\n",
    "new_names = list(set(fought_recently).difference(names))\n",
    "\n",
    "if len(new_names) != 0: # If the list is not empty\n",
    "    names += new_names # Append the new names\n",
    "    \n",
    "    # Overwrite the updated names list to the updates folder\n",
    "    with open(\"D:/data_projects/mma/updates/athletes_names_list.pickle\", \"wb\") as file:\n",
    "        pickle.dump(names, file)\n",
    "    \n",
    "    # Espn  \n",
    "    try:\n",
    "        new_fighters_espn_urls = []\n",
    "\n",
    "        for name in tqdm(new_names):\n",
    "\n",
    "            # Go to the url based on alphabet letter\n",
    "            main_link = 'http://www.espn.com/mma/fighters?search=' + name.split()[-1].lower()[0]\n",
    "            headers = {'User-agent': ''}\n",
    "            response = requests.get(main_link, headers=headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "            temp_list = [url for url in soup.find_all('a') if url is not None]\n",
    "            temp_list = ['https://www.espn.com' + url.get('href') for url in temp_list if '/mma/fighter/_/id/' in url.get('href')]\n",
    "\n",
    "            temp_names = ['-'.join(name.lower().split()) for name in new_names]\n",
    "\n",
    "            temp_links = [link for link in temp_list if any(name in link for name in temp_names)]\n",
    "\n",
    "            new_fighters_espn_urls += temp_links\n",
    "    \n",
    "        with open(\"D:/data_projects/mma/updates/athletes_espn_url.pickle\", \"rb\") as file:\n",
    "            temp_espn_url = pickle.load(file)\n",
    "            \n",
    "        temp_espn_url += new_fighters_espn_urls\n",
    "        \n",
    "        with open(\"D:/data_projects/mma/updates/athletes_espn_url.pickle\", \"wb\") as file:\n",
    "            pickle.dump(temp_espn_url, file)\n",
    "    except:\n",
    "        pass \n",
    "    \n",
    "#############################################################################################################################\n",
    "\n",
    "# Create links\n",
    "\n",
    "# Espn has the non static data which are the ones we will scrape frequently\n",
    "\n",
    "with open(\"D:/data_projects/mma/updates/athletes_espn_url.pickle\", \"rb\") as file:\n",
    "    espn_url = pickle.load(file)\n",
    "    \n",
    "temp_list = ['-'.join(name.lower().split()) for name in fought_recently]\n",
    "\n",
    "recent_espn_url = [url for url in espn_url if any(name in url for name in temp_list)]\n",
    "\n",
    "espn_df = pd.concat(list(map(get_fighters_df, recent_espn_url)))\n",
    "espn_df = espn_df[espn_df['Event'].str.contains('UFC')]\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "# Fix excessive columns\n",
    "\n",
    "excessive_columns = [col for col in espn_df.columns if (len(col.split('_')) > 1) and (len(col.split('_')[1]) == 1)]\n",
    "\n",
    "# If more columns got generated than we wanted\n",
    "if len(excessive_columns) != 0 :\n",
    "        \n",
    "    for col_to_fix in excessive_columns:\n",
    "    \n",
    "        espn_df[col_to_fix.split('_')[0]] = fixing_collumns(col_to_fix.split('_')[0], col_to_fix, espn_df)\n",
    "\n",
    "    # Drop the columns we dont need anymore\n",
    "    espn_df.drop(excessive_columns, axis=1, inplace = True)\n",
    "    \n",
    "#############################################################################################################################\n",
    "\n",
    "# UFC site\n",
    "\n",
    "ufc_url_list = ['https://www.ufc.com/athlete/' + '-'.join(name.lower().split()) for name in fought_recently]\n",
    "\n",
    "ufc_df = pd.concat(list(map(get_stats_ufc, tqdm(ufc_url_list))), ignore_index = True)\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "# UFCstats site\n",
    "\n",
    "ufcstats_list = []\n",
    "\n",
    "for name in fought_recently:\n",
    "\n",
    "    # Go to the url based on alphabet letter\n",
    "    main_link = 'http://ufcstats.com/statistics/fighters?char=' + name.split()[-1].lower()[0] + '&page=all'\n",
    "    headers = {'User-agent': ''}\n",
    "    response = requests.get(main_link, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        # Find the rows which have names and links\n",
    "        temp_list = ([row.find_all('a', {'class': 'b-link b-link_style_black'}) \n",
    "                      for row in soup.find_all('tr', {'class': 'b-statistics__table-row'})])\n",
    "        # And are not empty\n",
    "        temp_list = [sublist for sublist in temp_list if len(sublist) > 0]\n",
    "\n",
    "        # Match the name we are looking for with its link\n",
    "        html = [sublist for sublist in temp_list if sublist[0].text + ' ' + sublist[1].text == name][0][0]\n",
    "\n",
    "        # Get the link and append it to a list\n",
    "        soup = BeautifulSoup(str(html), 'html.parser')\n",
    "        link = soup.a.get('href')\n",
    "        ufcstats_list.append(link)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    sleep(3 + random.random())\n",
    "\n",
    "\n",
    "ufcstats_df = pd.concat(list(map(get_stats_ufcstats, ufcstats_list)), ignore_index = True)\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "# Combine dataframes\n",
    "\n",
    "# We do outer merge to keep all information and combine the same columns\n",
    "df = pd.merge(\n",
    "        espn_df, pd.merge(ufc_df, ufcstats_df, how='outer', on='name'),\n",
    "            how='outer', on='name'\n",
    "        )\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "# Cleaning\n",
    "\n",
    "# There are some '-', '--' values in our dataframe as there were on the sites which where scraped, we'll replace them with nan\n",
    "\n",
    "df.replace('-', np.nan, inplace=True)\n",
    "df.replace('--', np.nan, inplace=True)\n",
    "\n",
    "df = df[~df['division'].isnull()] # Remove the one row with no information\n",
    "\n",
    "# Change height from feet and inches to meters\n",
    "df['height'] = (df['height'].str.replace('\\'', '').str.split(' ').str[0].astype(float) * 0.3048 + \n",
    "                df['height'].str.replace('\"', '').str.split(' ').str[1].astype(float) * 0.0254)\n",
    "\n",
    "df['weight'] = df['weight'].str.split(' ').str[1] # Remove lbs from weight\n",
    "\n",
    "# Keep only the birth year\n",
    "df['birth_date'] = df['birth_date'].str.split('/').str[-1].str.split(' ').str[0].astype(float).round().astype(pd.Int64Dtype())\n",
    "\n",
    "df['reach'] = df['reach'].str[:-1].astype(float) * 0.0254 # Convert reach from inches to meters\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m %d, %Y', infer_datetime_format=True) # Convert to year-month-day\n",
    "\n",
    "\n",
    "df['TSL-TSA'] = df['TSL-TSA'].str[:-1] # Remove %\n",
    "df.rename(columns={\"TSL-TSA\": \"%TSL-TSA\"}, inplace=True) # Rename the column\n",
    "\n",
    "df['TK ACC'] = df['TK ACC'].str[:-1] # Remove %\n",
    "df.rename(columns={\"TK ACC\": \"%TK ACC\"}, inplace=True) # Rename the column\n",
    "\n",
    "df['%BODY'] = df['%BODY'].str[:-1] # Remove %\n",
    "df['%HEAD'] = df['%HEAD'].str[:-1] # Remove %\n",
    "df['%LEG'] = df['%LEG'].str[:-1] # Remove %\n",
    "\n",
    "# Time to seconds\n",
    "df['Time'] = df['Time'].str.split(':').str[0].astype(float) * 60 + df['Time'].str.split(':').str[1].astype(float) \n",
    "\n",
    "df['height_x'] = df['height_x'].astype(float) * 0.0254 # Inches to meters\n",
    "df['reach_x'] = df['reach_x'].astype(float) * 0.0254 # Inches to meters\n",
    "df['leg_reach'] = df['leg_reach'].astype(float) * 0.0254 # Inches to meters\n",
    "\n",
    "# Change height to meters\n",
    "df['height_y'] = (df['height_y'].str.replace('\\'', '').str.split(' ').str[0].astype(float) * 0.3048 + \n",
    "                df['height_y'].str.replace('\"', '').str.split(' ').str[1].astype(float) * 0.0254)\n",
    "\n",
    "df['weight_y'] = df['weight_y'].str.split().str[0].astype(float) # Remove lbs.\n",
    "df['reach_y'] = df['reach_y'].str.replace('\"', '').astype(float) * 0.0254 # Remove '\"' and convert to meters\n",
    "\n",
    " # Convert to year-month-day\n",
    "df['date_of_birth'] = df['date_of_birth'].str.split().str[-1].astype(float).round().astype(pd.Int64Dtype())\n",
    "\n",
    "\n",
    "# Fix the division column\n",
    "\n",
    "weight_classes = [\n",
    "            \"Light Heavyweight\", \"Featherweight\", \"Bantamweight\", \"Flyweight\",\n",
    "            \"Welterweight\", \"Women's Strawweight\", \"Women's Bantamweight\", \"Catchweight\"\n",
    "            \"Middleweight\", \"Lightweight\", \"Heavyweight\", \"Women's Featherweight\", \"Women's Flyweight\"]\n",
    "\n",
    "df['division'] = df['division'].where(df['division'].isin(weight_classes), other=np.nan)\n",
    "\n",
    "\n",
    "# Use combine_first to update null elements with values in the same location\n",
    "\n",
    "df['height'] = (df['height'].combine_first(df['height_x'])\n",
    "                            .combine_first(df['height_y'])) # Combine values of height, height_x, height_y\n",
    "\n",
    "df['weight'] = (df['weight'].combine_first(df['weight_x'])\n",
    "                            .combine_first(df['weight_y'])) # Combine values of weight, weight_x, weight_y\n",
    "\n",
    "df['birth_date'] = df['birth_date'].combine_first(df['date_of_birth']) # Combine birth dates\n",
    "\n",
    "df['stance_x'] = df['stance_x'].combine_first(df['stance_y']) # Combine stances\n",
    "df.rename(columns={'stance_x': 'stance'}, inplace=True) # Rename the column\n",
    "\n",
    "df['reach'] = (df['reach'].combine_first(df['reach_x'])\n",
    "                          .combine_first(df['reach_y'])) # Combine reaches\n",
    "\n",
    "\n",
    "\n",
    "# Drop the collumns which we dont need after combining them\n",
    "\n",
    "df.drop(['height_x', 'height_y', 'weight_x', 'weight_y', \n",
    "         'date_of_birth', 'stance_y', 'reach_x', 'reach_y'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Look at the documentation for explanation\n",
    "\n",
    "df['SDBL'] = df['SDBL/A'].str.split('/').str[0].astype(float).round().astype(pd.Int64Dtype())\n",
    "df['SDBA'] = df['SDBL/A'].str.split('/').str[1].astype(float).round().astype(pd.Int64Dtype())\n",
    "df['SDBL/A'] = df['SDBL'] / df['SDBA']\n",
    "\n",
    "df['SDHL'] = df['SDHL/A'].str.split('/').str[0].astype(float).round().astype(pd.Int64Dtype())\n",
    "df['SDHA'] = df['SDHL/A'].str.split('/').str[1].astype(float).round().astype(pd.Int64Dtype())\n",
    "df['SDHL/A'] = df['SDHL'] / df['SDHA']\n",
    "\n",
    "df['SDLL'] = df['SDLL/A'].str.split('/').str[0].astype(float).round().astype(pd.Int64Dtype())\n",
    "df['SDLA'] = df['SDLL/A'].str.split('/').str[1].astype(float).round().astype(pd.Int64Dtype())\n",
    "df['SDLL/A'] = df['SDLL'] / df['SDLA'] \n",
    "\n",
    "\n",
    "list_floats = ['weight', '%TSL-TSA', '%BODY' , '%HEAD', '%LEG', '%TK ACC']\n",
    "\n",
    "list_integers = ['TSL', 'TSA', 'SSL', 'SSA', 'KD', 'SCBL', 'SCBA', 'SCHL', 'SCHA', 'SCLL',\n",
    "                 'SCLA', 'RV', 'SR', 'TDL', 'TDA', 'TDS', 'SGBL', 'SGBA', 'SGHL', 'SGHA',\n",
    "                 'SGLL', 'SGLA', 'AD', 'ADTB', 'ADHG', 'ADTM', 'ADTS', 'SM', 'Rnd']\n",
    "\n",
    "# Convert floats\n",
    "df[list_floats] = df[list_floats].astype(float)\n",
    "\n",
    "# Convert integers\n",
    "df[list_integers] = df[list_integers].astype(float).round().astype(pd.Int64Dtype())\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "# Create an unique ID\n",
    "df['ID'] = df['Date'].astype(str) + '__' + df['name'] +  '__' + df['Opponent']\n",
    "\n",
    "# We want the ID to be the first column in our dataframe\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "\n",
    "df = df[cols]\n",
    "\n",
    "# Replace nans with None\n",
    "df = df.replace({pd.NaT: None})\n",
    "\n",
    "# Drop duplicates in ID\n",
    "df.drop_duplicates(subset='ID', keep=\"first\", inplace=True)\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "# 3 months ago date\n",
    "\n",
    "past_date = datetime.datetime.now() - datetime.timedelta(3 * 30)\n",
    "past_date = past_date.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Query data from the past 3 months\n",
    "\n",
    "conn = pg2.connect(host=\"localhost\", database='PostgreSQL_mma', user='postgres', password='password')\n",
    "\n",
    "\n",
    "data = pd.read_sql_query(f\"\"\"\n",
    "SELECT *\n",
    "FROM mma\n",
    "WHERE Date > '{past_date}'\n",
    "\n",
    "\"\"\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "\n",
    "# We want the ID to not be in the database (new fight), but because we queried the past 3 months,\n",
    "# we need to exclude older fights from the dataframe below\n",
    "# So in the past 3 months these are the fights done that are not in the database\n",
    "# Keep in mind the data we want are about 2 months old right now\n",
    "\n",
    "df = df[(~df['ID'].isin(data['id'])) & (df['Date'] > past_date)]\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "# Insert data in the database\n",
    "\n",
    "# Convert the DataFrame to a list of tuples\n",
    "data_to_db = [tuple(x) for x in df.to_numpy()]\n",
    "\n",
    "# %s ,%s ,%s ,%s , etc.. we use this in our query below\n",
    "values = ('%s ,'*len(df.columns))[:-2] \n",
    "\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "conn = pg2.connect(host=\"localhost\", database='PostgreSQL_mma', user='postgres', password='password')\n",
    "\n",
    "# Create a cursor object to perform database operations\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Insert data to the table\n",
    "insert_data(cur=cur, data=data_to_db, values=values)\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and database connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "#############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec26621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c60137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50125b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
